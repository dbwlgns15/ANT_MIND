{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9285d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import pickle\n",
    "from konlpy.tag import Okt\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "from transformers import TextClassificationPipeline\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertForSequenceClassification\n",
    "from datetime import date\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bda7e966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "loaded_tokenizer = BertTokenizerFast.from_pretrained('./src/bert', from_pt=True)\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained('./src/bert', from_pt=True)\n",
    "classifier = TextClassificationPipeline(tokenizer=loaded_tokenizer, model=loaded_model,\n",
    "                                            framework='tf', return_all_scores=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9019111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code(symbol):\n",
    "    krx = pd.read_csv('./src/krx_code.csv')\n",
    "    krx = krx.set_index('한글 종목약명')\n",
    "    try:\n",
    "        code = krx.at[symbol,'단축코드']\n",
    "        return code\n",
    "    except:\n",
    "        print('종목명을 다시 확인해주세요.')\n",
    "        return 0\n",
    "\n",
    "def get_today():\n",
    "    today = date.today().isoformat()\n",
    "    return today\n",
    "\n",
    "def get_comment(symbol):\n",
    "    code = get_code(symbol)\n",
    "    today = get_today()\n",
    "    comment_list = []\n",
    "    raw_comment_list = []\n",
    "    chk = 1\n",
    "    i = 1\n",
    "    while chk:  \n",
    "        url = f'https://finance.naver.com/item/board.naver?code={code}&page={i}'\n",
    "        headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36 Edg/100.0.1185.50'}\n",
    "        res = requests.get(url, headers = headers)\n",
    "        bs = BeautifulSoup(res.text, 'html.parser')  \n",
    "        for j in range(20):\n",
    "            try:\n",
    "                root = bs.find('div',{'class':'section inner_sub'}).find_all('tr',{'onmouseover':'mouseOver(this)'})[j].text.split('\\n') \n",
    "                if today != root[1].split()[0].replace('.','-'):\n",
    "                    chk = 0\n",
    "                    break\n",
    "                if len(root) == 14: # 답글\n",
    "                    pass      \n",
    "                elif len(root) == 13: # 기본\n",
    "                    comment = root[3]\n",
    "                    raw_comment_list.append(comment)\n",
    "                    comment = re.sub('\\[삭제된 게시물의 답글\\]',' ',comment)\n",
    "                    comment = re.sub('[^가-힣]',' ',comment)\n",
    "                    comment = re.sub(' +',' ',comment)\n",
    "                    comment = comment.strip()\n",
    "                    if comment == '':\n",
    "                        pass\n",
    "                    else:\n",
    "                        comment_list.append(comment)                 \n",
    "                else: # 에러\n",
    "                    pass\n",
    "            except: # 에러\n",
    "                pass\n",
    "            print(f'\\r{today} 댓글{len(comment_list)}개 크롤링중..',end='')\n",
    "        i += 1\n",
    "        if chk == 0:\n",
    "            break   \n",
    "    print(f'\\r{today} 댓글{len(comment_list)}개 크롤링완료')\n",
    "    return comment_list, raw_comment_list\n",
    "\n",
    "def BERT_feargreed(symbol):\n",
    "    comment_list, raw_comment_list = get_comment(symbol)\n",
    "\n",
    "#     loaded_tokenizer = BertTokenizerFast.from_pretrained('./src/bert', from_pt=True)\n",
    "#     loaded_model = TFBertForSequenceClassification.from_pretrained('./src/bert', from_pt=True)\n",
    "#     classifier = TextClassificationPipeline(tokenizer=loaded_tokenizer, model=loaded_model,\n",
    "#                                             framework='tf', return_all_scores=True)    \n",
    "    pred_list=[]\n",
    "    for i in raw_comment_list[:50]:\n",
    "        f = classifier(i)[0][0]['score']\n",
    "        g = classifier(i)[0][1]['score']\n",
    "        if f >= g:\n",
    "            pred_list.append(1-f)\n",
    "        else:\n",
    "            pred_list.append(g)\n",
    "        print(f'\\rBERT모델 댓글{len(pred_list)}개 분석중..',end='')\n",
    "    print(f'\\r{symbol} BERT 공포탐욕지수: {int(sum(pred_list)/len(pred_list)*100)}%')    \n",
    "    return comment_list\n",
    "\n",
    "def konlpy_okt(symbol):\n",
    "    okt = Okt()\n",
    "    tag_list = ['Noun','Verb','Adjective','VerbPrefix'] \n",
    "    comment_list = BERT_feargreed(symbol)\n",
    "    print('분석 진행중..',end='')\n",
    "    tokenized_data = []\n",
    "    for i in range(len(comment_list)):\n",
    "        tokenized_sentence = okt.pos(comment_list[i], stem=True) \n",
    "        tag_checked_sentence = []\n",
    "        for j in tokenized_sentence:\n",
    "            x,y = j\n",
    "            if y in tag_list:\n",
    "                tag_checked_sentence.append(x)\n",
    "        if tag_checked_sentence == []:\n",
    "            pass\n",
    "        else:\n",
    "            tokenized_data.append(tag_checked_sentence)     \n",
    "    for i in tokenized_data:\n",
    "        for j in range(len(i)):\n",
    "            i[j] = \"'\"+i[j]+\"'\"\n",
    "    return tokenized_data\n",
    "    \n",
    "def tokenize(symbol):\n",
    "    with open('./src/tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)   \n",
    "    tokenized_data = konlpy_okt(symbol)\n",
    "    test = tokenizer.texts_to_sequences(tokenized_data)\n",
    "    test = pad_sequences(test, maxlen=15)\n",
    "    return test\n",
    "\n",
    "def feargreed_index(symbol):\n",
    "    if get_code(symbol) == 0:\n",
    "        return    \n",
    "    model = load_model('./src/model.h5')\n",
    "    test = tokenize(symbol)\n",
    "    pred = model.predict(test)\n",
    "    print(f'\\r{symbol} LSTM 공포탐욕지수: {int(pred.mean()*100)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d930124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-25 댓글153개 크롤링완료.\n",
      "카카오 BERT 공포탐욕지수: 40%\n",
      "카카오 LSTM 공포탐욕지수: 20%\n"
     ]
    }
   ],
   "source": [
    "feargreed_index('카카오')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c8167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
